{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Task 2a: Text summarization with small files with Titan Text Premier\n",
    "\n",
    "\n",
    "In this notebook, you ingest a small string of text directly into the Amazon Bedrock API (using the Titan Text model) and instruct it to summarize the input text. You can apply this approach to summarize call transcripts, meeting transcripts, books, articles, blog posts, and other relevant content when the input text length is within the context size limits of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28279c",
   "metadata": {},
   "source": [
    "## Task 2a.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66edf151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4d9ee",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2a.2: Writing prompt with text to be summarized\n",
    "\n",
    "In this task, you use a short passage of text with fewer tokens than the maximum length supported by the foundation model. As a sample input text for this lab, you use a paragraph from an [AWS blog post](https://aws.amazon.com/jp/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/) announcing Amazon Bedrock.\n",
    "\n",
    "The prompt starts with an instruction `Please provide a summary of the following text.`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece0c069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "\n",
    "Please provide a summary of the following text:\n",
    "\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we’re also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock’s serverless experience, \\\n",
    "customers can easily find the right model for what they’re trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efddbb0",
   "metadata": {},
   "source": [
    "## Task 2a.3: Creating request body with prompt and inference parameters \n",
    "\n",
    "In this task, you create the request body with the above prompt and inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d191eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request body\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data, \n",
    "    \"textGenerationConfig\":{\n",
    "        \"maxTokenCount\":2048,\n",
    "        \"stopSequences\":[],\n",
    "        \"temperature\":0,\n",
    "        \"topP\":0.9\n",
    "        }\n",
    "    }) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f3326",
   "metadata": {},
   "source": [
    "## Task 2a.4: Invoke foundation model via Boto3\n",
    "\n",
    "In this task, you send an API request to Amazon Bedrock specifying the request parameters: `modelId`, `accept`, and `contentType`. Following the provided prompt, the foundation model in Amazon Bedrock then summarizes the input text.\n",
    "\n",
    "### Complete Output Generation\n",
    "\n",
    "By default, the Amazon Bedrock service generates the entire summary for a given prompt in a single output. This can be slow if the model output contains many tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f400d76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS has announced Amazon Bedrock, a new service that provides access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon via an API. Bedrock aims to make it easy for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders. The service offers a range of powerful FMs for text and images, including Amazon's Titan FMs, which consist of two new large language models (LLMs) that AWS is also announcing today. Bedrock's serverless experience allows customers to easily find the right model for their needs, customize FMs with their own data, and integrate and deploy them into their applications using familiar AWS tools and capabilities, without having to manage any infrastructure.\n"
     ]
    }
   ],
   "source": [
    "#model configuration and invoke the model\n",
    "modelId = 'amazon.titan-text-premier-v1:0' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "outputText = \"\\n\"\n",
    "\n",
    "try:\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error\n",
    "\n",
    "print(outputText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c84a0",
   "metadata": {},
   "source": [
    "### Streaming Output Generation\n",
    "\n",
    "Next, you explore how to use Amazon Bedrock's invoke_model_with_response_stream API to stream model outputs so users can consume outputs as they are generated. Rather than generating the full output at once, this API returns a ResponseStream that sends smaller output chunks from the model as they are produced. You can display these streaming outputs in a continuous, consumable view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3aa446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': {'bytes': b'{\"outputText\":\"AWS\",\"index\":0}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" has\",\"index\":1}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" announced\",\"index\":2}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Amazon\",\"index\":3}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Bed\",\"index\":4}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"rock\",\"index\":5}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":6}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" a\",\"index\":7}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" new\",\"index\":8}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" service\",\"index\":9}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" that\",\"index\":10}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" provides\",\"index\":11}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" access\",\"index\":12}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" to\",\"index\":13}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" foundation\",\"index\":14}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" models\",\"index\":15}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" (\",\"index\":16}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"FM\",\"index\":17}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":18}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\")\",\"index\":19}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" from\",\"index\":20}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" AI\",\"index\":21}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"2\",\"index\":22}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"1\",\"index\":23}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Labs\",\"index\":24}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":25}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" An\",\"index\":26}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"thro\",\"index\":27}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"pic\",\"index\":28}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":29}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Stability\",\"index\":30}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" AI\",\"index\":31}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":32}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":33}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Amazon\",\"index\":34}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" via\",\"index\":35}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" an\",\"index\":36}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" API\",\"index\":37}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\".\",\"index\":38}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Bed\",\"index\":39}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"rock\",\"index\":40}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" aims\",\"index\":41}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" to\",\"index\":42}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" make\",\"index\":43}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" it\",\"index\":44}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" easy\",\"index\":45}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" for\",\"index\":46}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" customers\",\"index\":47}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" to\",\"index\":48}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" build\",\"index\":49}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":50}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" scale\",\"index\":51}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" gene\",\"index\":52}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"rative\",\"index\":53}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" AI\",\"index\":54}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"-\",\"index\":55}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"based\",\"index\":56}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" applications\",\"index\":57}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" using\",\"index\":58}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" FM\",\"index\":59}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":60}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":61}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" \",\"index\":62}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"democrat\",\"index\":63}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"izing\",\"index\":64}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" access\",\"index\":65}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" for\",\"index\":66}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" all\",\"index\":67}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" builders\",\"index\":68}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\".\",\"index\":69}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" The\",\"index\":70}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" service\",\"index\":71}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" offers\",\"index\":72}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" a\",\"index\":73}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" range\",\"index\":74}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" of\",\"index\":75}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" powerful\",\"index\":76}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" FM\",\"index\":77}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":78}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" for\",\"index\":79}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" text\",\"index\":80}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":81}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" images\",\"index\":82}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":83}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" including\",\"index\":84}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Amazon\",\"index\":85}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"\\'\",\"index\":86}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":87}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Titan\",\"index\":88}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" FM\",\"index\":89}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":90}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":91}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" which\",\"index\":92}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" consist\",\"index\":93}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" of\",\"index\":94}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" two\",\"index\":95}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" new\",\"index\":96}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" large\",\"index\":97}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" language\",\"index\":98}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" models\",\"index\":99}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" (\",\"index\":100}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"LL\",\"index\":101}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"M\",\"index\":102}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":103}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\")\",\"index\":104}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" announced\",\"index\":105}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" today\",\"index\":106}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\".\",\"index\":107}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" Bed\",\"index\":108}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"rock\",\"index\":109}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"\\'\",\"index\":110}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":111}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" server\",\"index\":112}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"less\",\"index\":113}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" experience\",\"index\":114}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" allows\",\"index\":115}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" customers\",\"index\":116}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" to\",\"index\":117}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" easily\",\"index\":118}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" find\",\"index\":119}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" the\",\"index\":120}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" right\",\"index\":121}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" model\",\"index\":122}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":123}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" customize\",\"index\":124}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" FM\",\"index\":125}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"s\",\"index\":126}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" with\",\"index\":127}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" their\",\"index\":128}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" own\",\"index\":129}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" data\",\"index\":130}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":131}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":132}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" integrate\",\"index\":133}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":134}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" deploy\",\"index\":135}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" them\",\"index\":136}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" into\",\"index\":137}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" their\",\"index\":138}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" applications\",\"index\":139}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" using\",\"index\":140}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" familiar\",\"index\":141}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" AWS\",\"index\":142}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" tools\",\"index\":143}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" and\",\"index\":144}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" capabilities\",\"index\":145}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\",\",\"index\":146}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" without\",\"index\":147}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" managing\",\"index\":148}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" any\",\"index\":149}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\" infrastructure\",\"index\":150}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\".\",\"index\":151}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"\",\"completionReason\":\"FINISH\",\"totalOutputTextTokenCount\":152,\"index\":152,\"amazon-bedrock-invocationMetrics\":{\"inputTokenCount\":234,\"outputTokenCount\":152,\"invocationLatency\":3194,\"firstByteLatency\":659}}'}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke model with response stream\n",
    "modelId = 'amazon.titan-text-premier-v1:0'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "stream = response.get('body')\n",
    "output = list(stream)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ab3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown,Markdown,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0148858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ThrottlingException",
     "evalue": "An error occurred (ThrottlingException) when calling the InvokeModelWithResponseStream operation (reached max retries: 4): Too many requests, please wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mThrottlingException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m modelId \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon.titan-text-premier-v1:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model_with_response_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontentType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m stream \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:570\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/context.py:124\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    123\u001b[0m     hook()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:1031\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m     )\n\u001b[1;32m   1030\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mThrottlingException\u001b[0m: An error occurred (ThrottlingException) when calling the InvokeModelWithResponseStream operation (reached max retries: 4): Too many requests, please wait before trying again."
     ]
    }
   ],
   "source": [
    "modelId = 'amazon.titan-text-premier-v1:0'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "stream = response.get('body')\n",
    "output = []\n",
    "i = 1\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "            text = chunk_obj['outputText']\n",
    "            clear_output(wait=True)\n",
    "            output.append(text)\n",
    "            display_markdown(Markdown(''.join(output)))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8ee83",
   "metadata": {},
   "source": [
    "You have now experimented with using the boto3 SDK to access the Amazon Bedrock API. This SDK provides basic programmatic access to Bedrock capabilities. By leveraging this API, you were able to implement two use cases: 1) Generating an entire text summary of AWS news content at once, and 2) Streaming the summary output in chunks for incremental processing.\n",
    "\n",
    "### Try it yourself\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task2b.ipynb**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "00878cbed564b904a98b4a19808853cb6b9988746b881ea025a8408713879bf5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
